{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import time, utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:\n",
      " 0    10.0\n",
      "1     8.0\n",
      "2     4.0\n",
      "3     5.0\n",
      "4     5.0\n",
      "5     5.0\n",
      "Name: grade, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "full_data = pd.read_csv('train.csv', header=0)\n",
    "x_train, y_train = full_data['assignment'], full_data['grade']\n",
    "y_train = y_train.astype(float)\n",
    "print('y_train:\\n', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0]\n",
      " [0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n(We are using sklearn.feature_extraction.text.CountVectorizer instead)\\n\\n### Tokenize words \\n# tokenizer = Tokenizer(  num_words = 100_000,\\n#                         filters   = '$&()+/<=>[\\\\]^_`{|}~\\t')\\n# tokenizer.fit_on_texts(x_train) # Fit on the whole dataset because everything is available as we mark assignments\\n\\n# # Really tokenizing\\n# x_train_sequences = []\\n# for seq in tokenizer.texts_to_sequences_generator(x_train):\\n#     x_train_sequences.append(seq)\\n\\n# # Post-pad shorter sequences with 0 \\n# max_length = max([len(i) for i in x_train_sequences])\\n# x_train_tokenised = np.array(pad_sequences(x_train_sequences, maxlen=max_length, padding='post'))\\n\\n# print('----------\\nx_train_tokenised:\\n', x_train_tokenised)\\n# Output looks like this:\\n#   [[ 8  9 10 11 12  0  0  0  0  0  0]\\n#    [ 1  2  3  4  5 13 14  6  7 15 16]]\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = utils.clean_punctuation(x_train) # this also turns char to lowercase\n",
    "\n",
    "### Turn to frequency vector\n",
    "# convert each assignment into a feature vector representing the presence or absence of words from the vocabulary\n",
    "\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the assignments to build the vocabulary\n",
    "vectorizer.fit(x_train)\n",
    "\n",
    "# Transform the assignments into a matrix of feature vectors\n",
    "x_train_feature_vectors = vectorizer.transform(x_train)\n",
    "\n",
    "# Print the feature vectors\n",
    "print(x_train_feature_vectors.toarray())\n",
    "\n",
    "'''\n",
    "(We are using sklearn.feature_extraction.text.CountVectorizer instead)\n",
    "\n",
    "### Tokenize words \n",
    "# tokenizer = Tokenizer(  num_words = 100_000,\n",
    "#                         filters   = '$&()+/<=>[\\\\]^_`{|}~\\t')\n",
    "# tokenizer.fit_on_texts(x_train) # Fit on the whole dataset because everything is available as we mark assignments\n",
    "\n",
    "# # Really tokenizing\n",
    "# x_train_sequences = []\n",
    "# for seq in tokenizer.texts_to_sequences_generator(x_train):\n",
    "#     x_train_sequences.append(seq)\n",
    "\n",
    "# # Post-pad shorter sequences with 0 \n",
    "# max_length = max([len(i) for i in x_train_sequences])\n",
    "# x_train_tokenised = np.array(pad_sequences(x_train_sequences, maxlen=max_length, padding='post'))\n",
    "\n",
    "# print('----------\\nx_train_tokenised:\\n', x_train_tokenised)\n",
    "# Output looks like this:\n",
    "#   [[ 8  9 10 11 12  0  0  0  0  0  0]\n",
    "#    [ 1  2  3  4  5 13 14  6  7 15 16]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print out the frequency of each word in a sentence \n",
    "\n",
    "# vocabulary = vectorizer.get_feature_names()\n",
    "# # Get the frequency of each word in the sentence\n",
    "# word_frequencies = feature_vectors.toarray()[0]\n",
    "\n",
    "# # Print the word and its frequency\n",
    "# for word, frequency in zip(vocabulary, word_frequencies):\n",
    "#     print(f\"Word: {word}, Frequency: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_feature_vectors, y_train, test_size=0.2, random_state=42)\n",
    "# x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test: [array(['hi', 'rondo'], dtype='<U10'), array(['bla', 'blo', 'cluoud', 'discovered', 'gjango', 'hardware',\n",
      "       'phone', 'pi', 'react', 'tech', 'web'], dtype='<U10')]\n",
      "Predictions: [5. 4.]\n",
      "y_test: 0    10.0\n",
      "1     8.0\n",
      "Name: grade, dtype: float64\n",
      "Accuracy: 0.0\n",
      "Root Mean Squared Error (RMSE): 4.527692569068709\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "# Step 4: Train the Naive Bayes classifier and make predictions\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(x_train.toarray(), y_train)\n",
    "predictions = classifier.predict(x_test.toarray())\n",
    "\n",
    "print(f'x_test: {vectorizer.inverse_transform(x_test)}')\n",
    "print(f'Predictions: {predictions}')\n",
    "print(f'y_test: {y_test}')\n",
    "\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = mse ** 0.5\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "\n",
    "# Val\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Continual Learning?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
